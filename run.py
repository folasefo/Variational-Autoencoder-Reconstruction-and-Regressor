import os
import yaml
import argparse
import numpy as np
from pathlib import Path
from models import *
from experiment import VAEXperiment
import torch.backends.cudnn as cudnn
from pytorch_lightning import Trainer
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping
from pytorch_lightning import Callback
from dataset import VAEDataset
from pytorch_lightning.strategies import DDPStrategy
from pytorch_lightning import seed_everything
import torch
import re
import pytorch_lightning as pl
import copy


parser = argparse.ArgumentParser(description='Generic runner for VAE models')
parser.add_argument('--config', '-c',
                    dest="filename",
                    metavar='FILE',
                    help='path to the config file',
                    default='/home/wmee/PyTorch-VAE/configs/vae.yaml')

args = parser.parse_args()
with open(args.filename, 'r') as file:
    try:
        config = yaml.safe_load(file)
        max_epochs = config['trainer_params']['max_epochs']
        early_stop_params = config.get('early_stop_params', {})
        freeze_params = config.get('freeze_params', {})
        
        # è·å–é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        pretrain_params = config.get('pretrain_params', {})
        pretrained_path = pretrain_params.get('checkpoint_path', '')
    except yaml.YAMLError as exc:
        print(exc)

tb_logger = TensorBoardLogger(
    save_dir=config['logging_params']['save_dir'],
    name=config['model_params']['name'],
)

# For reproducibility
seed_everything(config['exp_params']['manual_seed'], True)

# åˆ›å»ºæ¨¡å‹
model = vae_models[config['model_params']['name']](**config['model_params'])

# ====== å…³é”®ä¿®æ”¹ï¼šåŠ è½½é¢„è®­ç»ƒå†»ç»“æ¨¡å‹ ======
if pretrained_path and os.path.exists(pretrained_path):
    print(f"ğŸš€ åŠ è½½é¢„è®­ç»ƒå†»ç»“æ¨¡å‹: {pretrained_path}")
    
    # åŠ è½½çŠ¶æ€å­—å…¸
    state_dict = torch.load(pretrained_path, map_location='cpu')
    
    # æå–æ¨¡å‹æƒé‡ï¼ˆå¯èƒ½åŒ…å«'state_dict'é”®ï¼‰
    if 'state_dict' in state_dict:
        state_dict = state_dict['state_dict']
    
    # è°ƒæ•´é”®åï¼šå»æ‰å‰ç¼€ï¼ˆä¾‹å¦‚ï¼š'model.'ï¼‰
    pretrained_dict = {}
    for k, v in state_dict.items():
        if k.startswith('model.'):
            k = k[6:]  # å»æ‰ 'model.' å‰ç¼€
        pretrained_dict[k] = v
    
    # åŠ è½½æƒé‡åˆ°å½“å‰æ¨¡å‹ï¼ˆä¸¥æ ¼æ¨¡å¼å…³é—­ï¼‰
    model.load_state_dict(pretrained_dict, strict=False)
    
    # å†»ç»“æŒ‡å®šæ¨¡å—
    freeze_encoder = config['freeze_params'].get('freeze_encoder', True)
    freeze_decoder = config['freeze_params'].get('freeze_decoder', True)
    
    if freeze_encoder:
        # å†»ç»“ç¼–ç å™¨å‚æ•°
        for param in model.encoder.parameters():
            param.requires_grad = False
        print("âœ… ç¼–ç å™¨å‚æ•°å·²å†»ç»“")
    
    if freeze_decoder:
        # å†»ç»“è§£ç å™¨å‚æ•°
        for param in model.decoder.parameters():
            param.requires_grad = False
        print("âœ… è§£ç å™¨å‚æ•°å·²å†»ç»“")
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params}/{total_params} ({trainable_params/total_params:.2%})")
else:
    print("âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")

config['exp_params']['label_csv'] = config['data_params']['label_csv']
config['exp_params']['bin_width'] = config['data_params'].get('bin_width', 0.1)
config['exp_params']['max_z'] = config['data_params'].get('max_z', 0.7)
# è¿™ä¸¤ä¸ªå¼€å…³/å¼ºåº¦ä¹Ÿä¼ ä¸€ä¸‹ï¼ˆè‹¥ YAML é‡Œæ²¡é…ï¼Œä¼šç”¨é»˜è®¤å€¼ï¼‰
config['exp_params']['use_bin_loss_weight'] = config['exp_params'].get('use_bin_loss_weight', True)
config['exp_params']['bin_weight_alpha']   = config['exp_params'].get('bin_weight_alpha', 0.5)

# åˆ›å»ºå®éªŒ
experiment = VAEXperiment(model, config['exp_params'])

data = VAEDataset(**config["data_params"], pin_memory=len(config['trainer_params']['gpus']) != 0)
data.setup()

# å®šä¹‰æ—©åœå›è°ƒ
early_stop_callback = EarlyStopping(
    monitor=early_stop_params.get('monitor', 'val_loss'),
    min_delta=early_stop_params.get('min_delta', 0.0005),
    patience=early_stop_params.get('patience', 15),
    verbose=early_stop_params.get('verbose', True),
    mode=early_stop_params.get('mode', 'min'),
)

# åˆ›å»ºModelCheckpointå›è°ƒå¹¶ä¿ç•™å¼•ç”¨
checkpoint_callback = ModelCheckpoint(
    save_top_k=2,
    dirpath=os.path.join(tb_logger.log_dir, "checkpoints"),
    monitor="val_loss",
    save_last=True,
    mode="min",
    filename='{epoch}-{val_loss:.4f}'  # æ–‡ä»¶ååŒ…å«epochå’Œlosså€¼
)

# ä¸ºsigma_NMADåˆ›å»ºå•ç‹¬çš„å›è°ƒ
sigma_nmad_callback = ModelCheckpoint(
    monitor="val_sigma_NMAD",
    dirpath=os.path.join(tb_logger.log_dir, "checkpoints"),
    filename="best_sigma_epoch={epoch}-sigma_NMAD={val_sigma_NMAD:.4f}",
    save_top_k=1,
    mode="min",
    save_last=False
)

class FreezeCallback(Callback):
    def __init__(self, freeze_modules=['encoder', 'decoder', 'fc_mu', 'fc_var'], save_frozen_model=True):
        super().__init__()
        self.freeze_modules = freeze_modules
        self.save_frozen_model = save_frozen_model
        self.best_sigma = float('inf')
        self.best_sigma_epoch = -1
        
    def on_validation_end(self, trainer, pl_module):
        """åœ¨æ¯ä¸ªéªŒè¯å‘¨æœŸç»“æŸæ—¶æ£€æŸ¥sigma_NMAD"""
        current_sigma = trainer.callback_metrics.get('val_sigma_NMAD', None)
        
        if current_sigma is not None and current_sigma < self.best_sigma:
            self.best_sigma = current_sigma
            self.best_sigma_epoch = trainer.current_epoch
            print(f"ğŸŒŸ æ–°çš„æœ€ä½³sigma_NMAD: {self.best_sigma:.4f} (epoch {self.best_sigma_epoch})")
            
            # ä¿å­˜æœ€ä½³sigma_NMADæ¨¡å‹
            ckpt_path = os.path.join(
                trainer.logger.log_dir, 
                "checkpoints", 
                f"best_sigma_epoch={self.best_sigma_epoch}-sigma_NMAD={self.best_sigma:.4f}.ckpt"
            )
            trainer.save_checkpoint(ckpt_path)
            print(f"ğŸ’¾ æœ€ä½³sigma_NMADæ¨¡å‹å·²ä¿å­˜è‡³: {ckpt_path}")
    
    def on_train_end(self, trainer, pl_module):
        """è®­ç»ƒç»“æŸåå†»ç»“æœ€ä½³sigma_NMADæ¨¡å‹çš„å‚æ•°"""
        if self.best_sigma_epoch == -1:
            print("âš ï¸ æœªæ£€æµ‹åˆ°æœ€ä½³sigma_NMADï¼Œè·³è¿‡å†»ç»“æ“ä½œ")
            return
            
        # åŠ è½½æœ€ä½³sigma_NMADæ¨¡å‹
        ckpt_path = os.path.join(
            trainer.logger.log_dir, 
            "checkpoints", 
            f"best_sigma_epoch={self.best_sigma_epoch}-sigma_NMAD={self.best_sigma:.4f}.ckpt"
        )
        
        if not os.path.exists(ckpt_path):
            print(f"âš ï¸ æ‰¾ä¸åˆ°æœ€ä½³sigma_NMADæ¨¡å‹: {ckpt_path}")
            return
            
        print(f"\nğŸ”¥ åœ¨æœ€ä½³sigma_NMADæ¨¡å‹ä¸Šå†»ç»“å‚æ•° (epoch {self.best_sigma_epoch}, Ïƒ_NMAD={self.best_sigma:.4f})")
        
        # åŠ è½½æ¨¡å‹çŠ¶æ€
        checkpoint = torch.load(ckpt_path, map_location='cpu')
        state_dict = checkpoint['state_dict']
        
        # ====== å…³é”®ä¿®å¤ï¼šå¤„ç†çŠ¶æ€å­—å…¸é”®å ======
        # åˆ›å»ºæ–°çš„çŠ¶æ€å­—å…¸ï¼Œç§»é™¤å¤šä½™çš„é”®å
        new_state_dict = {}
        for k, v in state_dict.items():
            # ç§»é™¤ 'model.' å‰ç¼€
            if k.startswith('model.'):
                k = k[6:]  # å»æ‰ 'model.' å‰ç¼€
            # å¿½ç•¥ num_batches_tracked å‚æ•°
            if 'num_batches_tracked' in k:
                continue
            new_state_dict[k] = v
        
        # åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹
        model = pl_module.model
        
        # åŠ è½½æƒé‡ï¼ˆä½¿ç”¨ strict=False å¿½ç•¥ä¸åŒ¹é…çš„é”®ï¼‰
        model.load_state_dict(new_state_dict, strict=False)
        print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        
        # å†»ç»“æŒ‡å®šæ¨¡å—
        for module_name in self.freeze_modules:
            module = getattr(model, module_name, None)
            if module is None:
                print(f"âš ï¸ æ‰¾ä¸åˆ°æ¨¡å— '{module_name}'ï¼Œè·³è¿‡å†»ç»“")
                continue
                
            for param in module.parameters():
                param.requires_grad = False
            print(f"âœ… æ¨¡å— '{module_name}' å·²å†»ç»“")
        
        # ä¿å­˜å†»ç»“çŠ¶æ€æ¨¡å‹
        if self.save_frozen_model:
            freeze_path = os.path.join(
                trainer.logger.log_dir, 
                "checkpoints", 
                f"frozen_sigma_epoch={self.best_sigma_epoch}-sigma_NMAD={self.best_sigma:.4f}.ckpt"
            )
            
            # ä¿å­˜å†»ç»“æ¨¡å‹
            torch.save({
                'state_dict': model.state_dict(),
                'hyper_parameters': {
                    'best_sigma': self.best_sigma,
                    'best_epoch': self.best_sigma_epoch,
                    'freeze_modules': self.freeze_modules
                }
            }, freeze_path)
            
            print(f"ğŸ’¾ å†»ç»“çŠ¶æ€æ¨¡å‹å·²ä¿å­˜è‡³: {freeze_path}")

# ä»é…ç½®è·å–å†»ç»“å‚æ•°
freeze_modules = freeze_params.get('freeze_modules', ['encoder', 'decoder', 'fc_mu', 'fc_var'])
save_frozen_model = freeze_params.get('save_frozen_model', True)

# åˆ›å»ºå†»ç»“å›è°ƒ
freeze_callback = FreezeCallback(
    freeze_modules=freeze_modules,
    save_frozen_model=save_frozen_model
)

# å…³é”®ä¿®æ”¹ï¼šæ›´æ–°DDPç­–ç•¥ä»¥æ”¯æŒå†»ç»“å‚æ•°
ddp_strategy = DDPStrategy(
    find_unused_parameters=True  # å…è®¸æœªä½¿ç”¨çš„å‚æ•°
)

runner = Trainer(
    logger=tb_logger,
    callbacks=[ 
        LearningRateMonitor(),
        checkpoint_callback,
        sigma_nmad_callback,
        early_stop_callback,
        freeze_callback
    ],
    strategy=ddp_strategy,
    replace_sampler_ddp=False,   # â˜… å…³é”®ï¼šè®©æˆ‘ä»¬åœ¨ DataModule é‡Œä¼ å…¥çš„ WeightedRandomSampler ç”Ÿæ•ˆ
    **config['trainer_params']
)

Path(f"{tb_logger.log_dir}/Samples").mkdir(exist_ok=True, parents=True)
Path(f"{tb_logger.log_dir}/Reconstructions").mkdir(exist_ok=True, parents=True)

print(f"======= Training {config['model_params']['name']} =======")
print(f"å†»ç»“æ¨¡å—: {freeze_modules}")
torch.cuda.empty_cache()
runner.fit(experiment, datamodule=data)

if hasattr(checkpoint_callback, 'best_model_path') and checkpoint_callback.best_model_path:
    best_model_filename = os.path.basename(checkpoint_callback.best_model_path)

    # è§£æ epoch
    epoch = "æœªçŸ¥"
    m_epoch = re.search(r'epoch=(\d+)', best_model_filename)
    if m_epoch:
        epoch = int(m_epoch.group(1))
    else:
        # å¯¹å½¢å¦‚ "51-0.0011.ckpt" çš„æ–‡ä»¶åï¼Œä» stem æŠ“å–
        stem = Path(best_model_filename).stem  # "51-0.0011"
        parts = stem.split('-')
        if parts and parts[0].isdigit():
            epoch = int(parts[0])

    # è§£æ val_loss
    best_loss = None
    m_loss = re.search(r'val_loss=([0-9]*\.?[0-9]+)', best_model_filename)
    if m_loss:
        best_loss = float(m_loss.group(1))
    else:
        # å¯¹å½¢å¦‚ "51-0.0011.ckpt"ï¼šå–å»åç¼€åçš„æœ€åä¸€æ®µ
        try:
            best_loss = float(Path(best_model_filename).stem.split('-')[-1])
        except Exception:
            # æœ€åå…œåº•
            best_loss = checkpoint_callback.best_model_score.item()

    print(f"\n{'='*50}")
    print(f"ğŸ† æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹ä¿¡æ¯:")
    print(f"ğŸ“ è·¯å¾„: {checkpoint_callback.best_model_path}")
    print(f"ğŸ”¢ Epoch: {epoch}")
    print(f"ğŸ“‰ æœ€ä½éªŒè¯Loss: {best_loss:.6f}")
    print(f"{'='*50}")

    with open(os.path.join(tb_logger.log_dir, "best_results.txt"), "w") as f:
        f.write(f"æœ€ä½³éªŒè¯æŸå¤±æ¨¡å‹è·¯å¾„: {checkpoint_callback.best_model_path}\n")
        f.write(f"æœ€ä½³Epoch: {epoch}\n")
        f.write(f"æœ€ä½éªŒè¯Loss: {best_loss:.6f}\n")
        
# æ˜¾ç¤ºæœ€ä½³sigma_NMADç»“æœ
if freeze_callback.best_sigma_epoch != -1:
    print(f"\n{'='*50}")
    print(f"ğŸ† æœ€ä½³sigma_NMADæ¨¡å‹ä¿¡æ¯:")
    print(f"ğŸ”¢ Epoch: {freeze_callback.best_sigma_epoch}")
    print(f"ğŸ“‰ æœ€ä½Ïƒ_NMAD: {freeze_callback.best_sigma:.6f}")
    print(f"ğŸ’¾ å†»ç»“æ¨¡å‹è·¯å¾„: {os.path.join(tb_logger.log_dir, 'checkpoints', f'frozen_sigma_epoch={freeze_callback.best_sigma_epoch}-sigma_NMAD={freeze_callback.best_sigma:.4f}.ckpt')}")
    print(f"{'='*50}")
    
    with open(os.path.join(tb_logger.log_dir, "best_results.txt"), "a") as f:
        f.write(f"\næœ€ä½³sigma_NMADæ¨¡å‹ä¿¡æ¯:\n")
        f.write(f"Epoch: {freeze_callback.best_sigma_epoch}\n")
        f.write(f"Ïƒ_NMAD: {freeze_callback.best_sigma:.6f}\n")
        f.write(f"å†»ç»“æ¨¡å‹è·¯å¾„: {os.path.join(tb_logger.log_dir, 'checkpoints', f'frozen_sigma_epoch={freeze_callback.best_sigma_epoch}-sigma_NMAD={freeze_callback.best_sigma:.4f}.ckpt')}\n")
else:
    print("âš ï¸ æœªæ‰¾åˆ°æœ€ä½³sigma_NMADæ¨¡å‹ä¿¡æ¯")

print("è®­ç»ƒå®Œæˆï¼")